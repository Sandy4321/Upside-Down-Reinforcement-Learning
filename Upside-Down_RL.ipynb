{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agents using Upside-Down Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See original papers:\n",
    "1. [Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions](https://arxiv.org/abs/1912.02875)\n",
    "2. [Training Agents using Upside-Down Reinforcement Learning](https://arxiv.org/abs/1912.02877)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content below is based on the [second paper](https://arxiv.org/abs/1912.02877), followed by my interpretation (implementation) of this content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "from helpers import make_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a rich history of techniques that incorporate supervised learning (SL) into reinforcement learning (RL) algorithms, it is believed that fully solving RL problems using SL is not possible, because feedback from the environment provides error signals in SL but evaluation signals in RL. Put simply, an agent gets feedback about how useful its actions are, but not about which actions are the best to take in any situation. On the possibility of turning an RL problem into an SL problem, Barto and Dietterich surmised: _\"In general, there is no way to do this.\"_\n",
    "\n",
    "In a companion technical report, Schmidhuber proposes to bridge this gap between SL and RL through UpsideDown Reinforcement Learning (⅂ꓤ), where environmental feedback – such as the reward – is an input rather than the learning target as in traditional RL algorithms based on reward prediction. Here we develop a practical RL algorithm for episodic tasks and show that it is indeed possible to train agents in general model-free settings without using value-based algorithms such as Q-learning, or policy-based ones such as policy gradients and evolutionary algorithms. Instead, RL uses pure SL to train an agent on all past experiences, and sidesteps the issues arising from the combination of function approximation, bootstrapping and off-policy training. We first describe its basic principles, then experimentally demonstrate its practical feasibility on three RL problems with both sparse and dense reward structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rl_vs_udrl.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/toy_env.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional model-free RL algorithms can be broadly classified as being value-based or policy-based. The core principle of value-based algorithms is reward prediction: agents are trained to predict the expected discounted future return for taking any action in any state, commonly using TD learning. Policy-based algorithms are instead based on directly searching for policies that maximize returns. The basic principle of ⅂ꓤ are different from both of these categories: given a particular definition of _commands_, it defines a _behavior function_ that encapsulates knowledge about the behaviors observed so far compatible with known commands. The nature of the behavior function is explained using two examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a simple dart-throwing environment where each episode lasts a single step. An agent learning to throw darts receives a return inversely proportional to the hit distance from the center of the board. In each episode, the agent observes the initial state of the dart, and takes an action that determines the force and direction of the throw. Using value-based RL for this task would amount to training the agent to predict the expected return for various actions and initial states. This knowledge would then be used for action selection e.g. taking the action with the highest expected return.\n",
    "\n",
    "In ⅂ꓤ, the agent’s knowledge is represented not in terms of expected returns for various states and actions, but in terms of actions that are compatible with various states and desired returns i.e. the inputs and targets of the agent’s learning procedure are switched. The dart throwing agent would be trained to directly produce the actions for hitting desired locations on the board, using a behavior function B<sup>2</sup> learned using its past experience. Figure 1 schematically illustrates this difference between B and the Q-value function commonly used in value based RL. Since this environment consists of episodes with a single time step, both Q and B can be learned using SL. \n",
    "\n",
    "The next example illustrates a slightly more typical RL setting with longer time horizons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the simple deterministic Markovian environment in Figure 2 in which all trajectories start in s<sub>0</sub> or s<sub>1</sub> and end in s<sub>2</sub> or s<sub>3</sub>. Additionally consider commands of the type: achieve a given desired return in a given desired horizon from the current state. A behavior function based on the set of all unique behaviors possible in this environment can then be expressed in a tabular form in Table 1. It maps states and commands to the action to be taken in that state compatible with executing the command. In other words, it answers the question: _\"if an agent is in a given state and desires a given return over a given horizon, which action should it take next?\"_ By design, this function can now be used to execute any valid command in the environment without further knowledge.\n",
    "\n",
    "Two properties of the behavior function are notable. First, the output of B can be stochastic even in a deterministic environment since there may be multiple valid behaviors compatible with the same command and state. For example, this would be the case if the transition s<sub>0</sub> → s<sub>2</sub> had a reward of 2. So in general, B produces a probability distribution over actions. Second, B fundamentally depends on the set of trajectories used to construct it. Using a loss function _L_, we define the optimal behavior function B<sup>∗</sup><sub>Т</sub> for a set of trajectories Т as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"images/optimal_B.png\" width=\"320\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here len(τ) is the length of any trajectory τ. For a suitably parameterized B, we use the cross-entropy between the observed and predicted distributions of actions as the loss function. Equivalently, we search for parameters that maximize the likelihood that the behavior function generates the available data, using the traditional tools of supervised learning. Similarly, we can define a behavior function over a policy. Instead of a set of trajectories, B<sup>∗</sup><sub>π</sub> minimizes the same loss over the distribution of trajectories generated when acting according to π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⅂ꓤ Algorithm for Maximizing Episodic Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, a behavior function can be learned for any policy that generates all possible trajectories in an environment given suficient time (e.g. a random policy) and then used to select actions that lead to any desired return in a desired horizon achievable in the environment. But such a learning procedure is not practical since it relies on undirected exploration using a fixed policy. Moreover, in environments with scalar rewards, the goal is to learn to achieve high returns and not to achieve any possible return over any horizon. Therefore, the concrete algorithm used in this paper trains a behavior function on the set of trajectories (or the agent’s experience) so far and incorporates minimal additions that enable the continual collection of trajectories with higher returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-level pseudo-code for the proposed algorithm is described in Algorithm 1. It starts by initializing an empty replay buffer to collect the agent’s experiences during training, and filling it with a few episodes of random interactions. The behavior function of the agent is continually improved by supervised training on previous experiences recorded in the replay buffer. After each training phase, the behavior function is used to act in the environment to obtain new experiences that are added to the replay buffer. This procedure continues until a stopping criterion is met, such as reaching the allowed maximum number of interactions with the environment. The remainder of this section describes each step of the algorithm and introduces the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_replay_buffer(replay_size, n_episodes, last_few):\n",
    "    random_policy = lambda state, command: np.random.randint(env.action_space.n)\n",
    "    \n",
    "    buffer = ReplayBuffer(replay_size)\n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset().tolist()\n",
    "        command = sample_command(buffer, last_few)\n",
    "        episode = generate_episode(random_policy, state, command) # See Algorithm 2\n",
    "        buffer.add(episode)\n",
    "    \n",
    "    buffer.sort()\n",
    "    return buffer\n",
    "\n",
    "def initialize_behavior_function(state_size, \n",
    "                                 action_size, \n",
    "                                 hidden_size, \n",
    "                                 learning_rate, \n",
    "                                 command_scale):\n",
    "    behavior = Behavior(state_size, \n",
    "                        action_size, \n",
    "                        hidden_size, \n",
    "                        command_scale)\n",
    "    \n",
    "    behavior.init_optimizer(lr=learning_rate)\n",
    "    \n",
    "    return behavior\n",
    "\n",
    "def generate_episodes(policy, buffer, n_episodes, last_few):\n",
    "    for i in range(n_episodes_per_iter):\n",
    "        command = sample_command(buffer, last_few)\n",
    "        state = env.reset().tolist()\n",
    "        episode = generate_episode(policy, state, command) # See Algorithm 2\n",
    "        buffer.add(episode)\n",
    "    \n",
    "    buffer.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UDRL():\n",
    "    '''Upside-Down Reinforcement Learning main algrithm'''\n",
    "    \n",
    "    buffer = initialize_replay_buffer(replay_size, \n",
    "                                      n_warm_up_episodes, \n",
    "                                      last_few)\n",
    "    behavior = initialize_behavior_function(state_size, \n",
    "                                            action_size, \n",
    "                                            hidden_size, \n",
    "                                            learning_rate, \n",
    "                                            [return_scale, horizon_scale])\n",
    "    \n",
    "    stochastic_policy = lambda state, command: behavior.action(state, command)\n",
    "    greedy_policy = lambda state, command: behavior.greedy_action(state, command)\n",
    "    \n",
    "    for i in range(1, n_main_iter+1):\n",
    "        mean_loss = train_behavior(behavior, buffer, n_updates_per_iter, batch_size)\n",
    "        \n",
    "        print('Iter: {}, Loss: {:.3f}'.format(i, mean_loss), end='\\r')\n",
    "        \n",
    "        # Sample exploratory commands and generate episodes\n",
    "        generate_episodes(stochastic_policy, \n",
    "                          buffer, \n",
    "                          n_episodes_per_iter,\n",
    "                          last_few)\n",
    "        \n",
    "        if i % evaluate_every == 0:\n",
    "            total_reward = evaluate_agent(greedy_policy, buffer, last_few)\n",
    "            \n",
    "            if total_reward >= target_return: \n",
    "                break\n",
    "    \n",
    "    return behavior, buffer, stochastic_policy, greedy_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "RL does not explicitly maximize returns, but instead relies on exploration to continually discover higher return trajectories so that the behavior function can be trained on them. To drive learning progress, we found it helpful to use a replay buffer containing a fixed maximum number of trajectories with the highest returns seen so far, sorted in increasing order by return. The maximum buffer size is a hyperparameter. Since the agent starts learning with zero experience, an initial set of trajectories is generated by executing random actions in the environment. The trajectories are added to the replay buffer and used to start training the agent’s behavior function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, episode):\n",
    "        self.buffer.append(episode)\n",
    "    \n",
    "    def get(self, num):\n",
    "        return self.buffer[-num:]\n",
    "    \n",
    "    def random_batch(self, batch_size):\n",
    "        idxs = np.random.randint(0, len(self), batch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "    \n",
    "    def sort(self):\n",
    "        key_sort = lambda episode: episode.total_return\n",
    "        self.buffer = sorted(self.buffer, key=key_sort)[-self.size:]\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Function and Setup\n",
    "\n",
    "As described earlier, at any time t during an episode, the current behavior function B produces an action distribution in response to the current state st and command c<sub>t</sub> = (d<sup>r</sup><sub>t</sub>, d<sup>h</sup><sub>t</sub>), where d<sup>r</sup><sub>t</sub> ∈ R is the desired return and d<sup>h</sup><sub>t</sub> ∈ N is the desired time horizon at time t. The predicted action distribution _P_(a<sub>t</sub> | s<sub>t</sub>, c<sub>t</sub>) = _B_(s<sub>t</sub>, c<sub>t</sub>; θ), where θ denotes a vector of trainable parameters, is expected to lead to successful execution of the command ct interpreted as: “achieve a return d<sup>r</sup><sub>t</sub> during the next d<sup>h</sup><sub>t</sub> steps”. For a given initial command input c<sub>0</sub>, _B_ can be used to generate a trajectory using Algorithm 2 by sampling actions predicted for the current command and updating the command according to the obtained rewards and elapsed time.\n",
    "\n",
    "An important implementation detail is that d<sup>h</sup><sub>t</sub> is always set to max(d<sup>h</sup><sub>t</sub>, 1) such that it is a valid time horizon. Furthermore, d<sup>r</sup><sub>t</sub> is clipped such that it is upper-bounded by the maximum return achievable in the environment. This only affects agent evaluations (not training) and avoids situations where negative rewards (rt) can lead to desired returns that are not achievable from any state (see Algorithm 2; line 8).\n",
    "\n",
    "All agents were implemented using articial neural networks. The behavior function for UDRL agents was implemented using fully-connected feed-forward networks for LunarLander-v2, and convolutional neural networks (CNNs; 16) for TakeCover-v0. The command inputs were scaled by a fixed scaling factor, transformed by a fully-connected sigmoidal layer, and then multiplied element-wise with an embedding of the observed inputs (after the first layer for fully-connected networks; after all convolutional layers for CNNs). Apart from this small modification regarding UDRL command inputs, the network architectures were identical for all algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(nn.Module):\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 hidden_size, \n",
    "                 command_scale = [1, 1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.command_scale = torch.FloatTensor(command_scale)\n",
    "        \n",
    "        self.state_fc = nn.Sequential(torch.nn.Linear(state_size, \n",
    "                                                      hidden_size), \n",
    "                                      nn.Sigmoid())\n",
    "        \n",
    "        self.command_fc = nn.Sequential(nn.Linear(2, hidden_size), \n",
    "                                        nn.Sigmoid())\n",
    "        \n",
    "        self.output_fc = nn.Sequential(nn.Linear(hidden_size, \n",
    "                                                 hidden_size), \n",
    "                                       nn.ReLU(), \n",
    "                                       nn.Linear(hidden_size, \n",
    "                                                 action_size))\n",
    "        \n",
    "    \n",
    "    def forward(self, state, command):\n",
    "        state_output = self.state_fc(state)\n",
    "        command_output = self.command_fc(command * self.command_scale)\n",
    "        embedding = torch.mul(state_output, command_output)\n",
    "        return self.output_fc(embedding)\n",
    "    \n",
    "    def action(self, state, command):\n",
    "        logits = self.forward(state, command)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        return dist.sample().item()\n",
    "    \n",
    "    def greedy_action(self, state, command):\n",
    "        logits = self.forward(state, command)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return np.argmax(probs.detach().numpy())\n",
    "    \n",
    "    def init_optimizer(self, optim = Adam, lr = 0.003):\n",
    "        self.optim = optim(self.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy, state, command = [1, 1]):\n",
    "    desired_return = command[0]\n",
    "    desired_horizon = command[1]\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    time_steps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(torch.FloatTensor(state), \n",
    "                        torch.FloatTensor(command))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        state = next_state.tolist()\n",
    "        \n",
    "        # Clipped such that it's upper-bounded by the maximum return achievable in the env\n",
    "        desired_return = min(desired_return - reward, max_reward)\n",
    "        \n",
    "        # Make sure it's always a valid horizon\n",
    "        desired_horizon = max(desired_horizon - 1, 1)\n",
    "        \n",
    "        command = [desired_return, desired_horizon]\n",
    "        time_steps += 1\n",
    "        \n",
    "    return make_episode(states, actions, rewards, sum(rewards), time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Behavior Function\n",
    "\n",
    "As discussed in Section 2.2, _B_ admits supervised training on a large amount of input-target examples from any past episode. The goal of training is to make the behavior function produce outputs consistent with all previously recorded trajectories in the replay buffer according to Equation 1. \n",
    "\n",
    "To draw a training example from a random episode in the replay buffer, time step indices t1 and t2 are selected\n",
    "randomly such that 0 ≤ t1 < t2 ≤ T, where T is the length of the selected episode. Then the input for training _B_ is\n",
    "(s<sub>t1</sub>, (d<sup>r</sup>, d<sup>h</sup>)), where d<sup>r</sup> = ∑ r<sub>t</sub> and d<sup>h</sup> = t2 − t1, and the target is a<sub>t1</sub>, the action taken at t1. To summarize, the training examples are generated by selecting the time horizons, actions, observations and rewards in the past, and generating input-target pairs consistent with them.\n",
    "\n",
    "Several heuristics may be used to select and combine training examples into mini-batches for gradient-based SL. For\n",
    "all experiments in this paper, only \"trailing segments\" were sampled from each episode, i.e., we set t2 = T − 1 where\n",
    "T is the length of any episode. This discards a large amount of potential training examples but is a good fit for episodic tasks where the goal is to optimize the total reward until the end of each episode. It also makes training easier, since the behavior function only needs to learn to execute a subset of possible commands. To keep the setup simple, a fixed number of training iterations using Adam were performed in each training step for all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_behavior(behavior, buffer, n_updates, batch_size):\n",
    "    all_loss = []\n",
    "    for update in range(n_updates):\n",
    "        episodes = buffer.random_batch(batch_size)\n",
    "        \n",
    "        batch_states = []\n",
    "        batch_commands = []\n",
    "        batch_actions = []\n",
    "        \n",
    "        for episode in episodes:\n",
    "            T = episode.length\n",
    "            t1 = np.random.randint(0, T)\n",
    "            # t2 = np.random.randint(t1+1, T+1)\n",
    "            t2 = T - 1\n",
    "            dr = sum(episode.rewards[t1:t2])\n",
    "            dh = t2 - t1\n",
    "            \n",
    "            st1 = episode.states[t1]\n",
    "            at1 = episode.actions[et1]\n",
    "            \n",
    "            batch_states.append(st1)\n",
    "            batch_actions.append(at1)\n",
    "            batch_commands.append([dr, dh])\n",
    "        \n",
    "        batch_states = torch.FloatTensor(batch_states)\n",
    "        batch_commands = torch.FloatTensor(batch_commands)\n",
    "        batch_actions = torch.LongTensor(batch_actions)\n",
    "        \n",
    "        pred = behavior(batch_states, batch_commands)\n",
    "        \n",
    "        loss = F.cross_entropy(pred, batch_actions)\n",
    "        \n",
    "        behavior.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        behavior.optim.step()\n",
    "        \n",
    "        all_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Exploratory Commands\n",
    "\n",
    "After each training phase, the agent can attempt to generate new, previously infeasible behavior, potentially achieving higher returns. To profit from such exploration through generalization, one must first create a set of new initial commands c0 to be used in Algorithm 2. We use the following procedure to sample commands:\n",
    "\n",
    "1. A number of episodes from the end of the replay buffer (i.e., with the highest returns) are selected. This number is a hyperparameter and remains fixed during training.\n",
    "\n",
    "2. The exploratory desired horizon d<sup>h</sup><sub>0</sub> is set to the mean of the lengths of the selected episodes.\n",
    "\n",
    "3. The exploratory desired returns d<sup>r</sup><sub>0</sub> are sampled from the uniform distribution U\\[M, M + S\\] where M is the mean and S is the standard deviation of the selected episodic returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_command(buffer, last_few):\n",
    "    if len(buffer) == 0: return [1, 1]\n",
    "    \n",
    "    # 1.\n",
    "    commands = buffer.get(last_few)\n",
    "    \n",
    "    # 2.\n",
    "    lengths = [command.length for command in commands]\n",
    "    desired_horizon = np.mean(lengths)\n",
    "    \n",
    "    # 3.\n",
    "    returns = [command.total_return for command in commands]\n",
    "    mean_return, std_return = np.mean(returns), np.std(returns)\n",
    "    desired_returns = np.random.uniform(mean_return, mean_return+std_return)\n",
    "    \n",
    "    return [desired_returns, desired_horizon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 2 is also used to evaluate the agent at any time using evaluation commands derived from the most recent\n",
    "exploratory commands. The initial desired return d<sup>r</sup><sub>0</sub> is set to the lower bound of the desired returns from the most recent exploratory command, and the initial desired horizon d<sup>h</sup><sub>0</sub> from the most recent exploratory command is reused. In certain conditions, greedy actions – using the mode of the action distribution – can also be used, but we omit this option here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this is very similar to algo 2. Combine\n",
    "def evaluate_agent(policy, buffer, last_few, render=False):\n",
    "\n",
    "    print('\\nEvaluation.', end=' ')\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset().tolist()\n",
    "    \n",
    "    command = sample_command(buffer, last_few)\n",
    "    desired_return = command[0]\n",
    "    desired_horizon = command[1]\n",
    "    \n",
    "    print('Desired return: {:.3f}, Desired horizon: {}.'.format(desired_return, desired_horizon), end=' ')\n",
    "    \n",
    "    while not done:\n",
    "        if render: env.render()\n",
    "            \n",
    "        action = policy(torch.FloatTensor(state), \n",
    "                        torch.FloatTensor(command))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state.tolist()\n",
    "        desired_return = min(desired_return - reward, max_reward)\n",
    "        desired_horizon = max(desired_horizon - 1, 1)\n",
    "        command = [desired_return, desired_horizon]\n",
    "    \n",
    "    if render: env.close()\n",
    "    \n",
    "    print('Total reward achieved: {:.3f}'.format(total_reward))\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import rocket_lander_gym\n",
    "\n",
    "env = gym.make('LunarLander-v2') # RocketLander-v0 | LunarLander-v2 | MountainCar-v0 | CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 8\n",
      "Action size: 4\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "print('State size: {}'.format(state_size))\n",
    "print('Action size: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upside-Down RL Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations in the main loop\n",
    "n_main_iter = 1000\n",
    "\n",
    "# Number of (input, target) pairs per batch used for training the behavior function\n",
    "batch_size = 1024\n",
    "\n",
    "# Scaling factor for desired horizon input\n",
    "horizon_scale = 0.01\n",
    "\n",
    "# Number of episodes from the end of the replay buffer used for sampling exploratory\n",
    "# commands\n",
    "last_few = 100\n",
    "\n",
    "# Learning rate for the ADAM optimizer\n",
    "learning_rate = 0.02\n",
    "\n",
    "# Number of exploratory episodes generated per step of UDRL training\n",
    "n_episodes_per_iter = 100\n",
    "\n",
    "# Number of gradient-based updates of the behavior function per step of UDRL training\n",
    "n_updates_per_iter = 15\n",
    "\n",
    "# Number of warm up episodes at the beginning of training\n",
    "n_warm_up_episodes = 100\n",
    "\n",
    "# Maximum size of the replay buffer (in episodes)\n",
    "replay_size = 500\n",
    "\n",
    "# Scaling factor for desired horizon input\n",
    "return_scale = 0.01\n",
    "\n",
    "# Evaluate the agent after evaluate_every iterations\n",
    "evaluate_every = 10\n",
    "\n",
    "# Target return before breaking out of the training loop\n",
    "target_return = 200\n",
    "\n",
    "# Maximun reward given by the environment\n",
    "max_reward = 200\n",
    "\n",
    "# Hidden units\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10, Loss: 1.386\n",
      "Evaluation. Desired return: -58.426, Desired horizon: 104.1. Total reward achieved: -182.852\n",
      "Iter: 20, Loss: 1.386\n",
      "Evaluation. Desired return: -63.794, Desired horizon: 111.57. Total reward achieved: -176.385\n",
      "Iter: 30, Loss: 1.387\n",
      "Evaluation. Desired return: -51.827, Desired horizon: 116.45. Total reward achieved: -131.552\n",
      "Iter: 40, Loss: 1.386\n",
      "Evaluation. Desired return: -42.092, Desired horizon: 127.61. Total reward achieved: -978.793\n",
      "Iter: 50, Loss: 1.387\n",
      "Evaluation. Desired return: -21.947, Desired horizon: 124.47. Total reward achieved: -499.622\n",
      "Iter: 60, Loss: 1.387\n",
      "Evaluation. Desired return: -16.282, Desired horizon: 122.97. Total reward achieved: -585.788\n",
      "Iter: 70, Loss: 1.386\n",
      "Evaluation. Desired return: -23.602, Desired horizon: 124.51. Total reward achieved: -1076.141\n",
      "Iter: 80, Loss: 1.386\n",
      "Evaluation. Desired return: -11.484, Desired horizon: 123.72. Total reward achieved: -1035.261\n",
      "Iter: 90, Loss: 1.386\n",
      "Evaluation. Desired return: -15.007, Desired horizon: 122.67. Total reward achieved: -509.782\n",
      "Iter: 100, Loss: 1.386\n",
      "Evaluation. Desired return: -12.834, Desired horizon: 139.32. Total reward achieved: -170.702\n",
      "Iter: 110, Loss: 1.386\n",
      "Evaluation. Desired return: -13.588, Desired horizon: 139.06. Total reward achieved: -571.668\n",
      "Iter: 120, Loss: 1.386\n",
      "Evaluation. Desired return: -4.685, Desired horizon: 140.2. Total reward achieved: -531.826\n",
      "Iter: 130, Loss: 1.386\n",
      "Evaluation. Desired return: -15.772, Desired horizon: 140.46. Total reward achieved: -813.461\n",
      "Iter: 140, Loss: 1.386\n",
      "Evaluation. Desired return: -3.908, Desired horizon: 148.36. Total reward achieved: -757.616\n",
      "Iter: 150, Loss: 1.386\n",
      "Evaluation. Desired return: -1.767, Desired horizon: 157.88. Total reward achieved: -198.968\n",
      "Iter: 160, Loss: 1.386\n",
      "Evaluation. Desired return: -4.785, Desired horizon: 157.89. Total reward achieved: -465.133\n",
      "Iter: 170, Loss: 1.387\n",
      "Evaluation. Desired return: -5.454, Desired horizon: 166.77. Total reward achieved: -176.355\n",
      "Iter: 180, Loss: 1.386\n",
      "Evaluation. Desired return: -4.010, Desired horizon: 175.86. Total reward achieved: -527.286\n",
      "Iter: 190, Loss: 1.387\n",
      "Evaluation. Desired return: 0.900, Desired horizon: 176.6. Total reward achieved: -673.293\n",
      "Iter: 200, Loss: 1.387\n",
      "Evaluation. Desired return: 4.025, Desired horizon: 176.21. Total reward achieved: -522.167\n",
      "Iter: 210, Loss: 1.386\n",
      "Evaluation. Desired return: 4.887, Desired horizon: 185.44. Total reward achieved: -559.574\n",
      "Iter: 220, Loss: 1.386\n",
      "Evaluation. Desired return: 2.591, Desired horizon: 184.44. Total reward achieved: -219.667\n",
      "Iter: 230, Loss: 1.386\n",
      "Evaluation. Desired return: -0.043, Desired horizon: 194.28. Total reward achieved: -170.025\n",
      "Iter: 240, Loss: 1.386\n",
      "Evaluation. Desired return: 4.372, Desired horizon: 212.51. Total reward achieved: -182.090\n",
      "Iter: 250, Loss: 1.386\n",
      "Evaluation. Desired return: 1.344, Desired horizon: 220.63. Total reward achieved: -566.276\n",
      "Iter: 260, Loss: 1.386\n",
      "Evaluation. Desired return: 10.698, Desired horizon: 229.74. Total reward achieved: -29.930\n",
      "Iter: 270, Loss: 1.386\n",
      "Evaluation. Desired return: 11.328, Desired horizon: 247.73. Total reward achieved: -491.125\n",
      "Iter: 280, Loss: 1.386\n",
      "Evaluation. Desired return: 11.409, Desired horizon: 247.9. Total reward achieved: -192.290\n",
      "Iter: 290, Loss: 1.386\n",
      "Evaluation. Desired return: 12.485, Desired horizon: 256.48. Total reward achieved: -672.893\n",
      "Iter: 300, Loss: 1.386\n",
      "Evaluation. Desired return: 11.094, Desired horizon: 265.87. Total reward achieved: -125.821\n",
      "Iter: 310, Loss: 1.386\n",
      "Evaluation. Desired return: 4.800, Desired horizon: 283.14. Total reward achieved: -221.918\n",
      "Iter: 320, Loss: 1.386\n",
      "Evaluation. Desired return: 7.961, Desired horizon: 282.85. Total reward achieved: -230.450\n",
      "Iter: 330, Loss: 1.386\n",
      "Evaluation. Desired return: 11.585, Desired horizon: 310.49. Total reward achieved: -184.872\n",
      "Iter: 340, Loss: 1.386\n",
      "Evaluation. Desired return: 17.299, Desired horizon: 319.64. Total reward achieved: -530.957\n",
      "Iter: 350, Loss: 1.386\n",
      "Evaluation. Desired return: 17.835, Desired horizon: 319.29. Total reward achieved: -147.013\n",
      "Iter: 360, Loss: 1.386\n",
      "Evaluation. Desired return: 15.946, Desired horizon: 319.03. Total reward achieved: -467.192\n",
      "Iter: 370, Loss: 1.386\n",
      "Evaluation. Desired return: 6.927, Desired horizon: 328.2. Total reward achieved: -180.199\n",
      "Iter: 380, Loss: 1.385\n",
      "Evaluation. Desired return: 12.958, Desired horizon: 335.9. Total reward achieved: -189.815\n",
      "Iter: 390, Loss: 1.386\n",
      "Evaluation. Desired return: 8.993, Desired horizon: 335.91. Total reward achieved: -662.896\n",
      "Iter: 400, Loss: 1.386\n",
      "Evaluation. Desired return: 18.976, Desired horizon: 354.1. Total reward achieved: -543.226\n",
      "Iter: 410, Loss: 1.386\n",
      "Evaluation. Desired return: 21.717, Desired horizon: 363.11. Total reward achieved: -172.147\n",
      "Iter: 420, Loss: 1.386\n",
      "Evaluation. Desired return: 15.250, Desired horizon: 362.47. Total reward achieved: -184.060\n",
      "Iter: 430, Loss: 1.387\n",
      "Evaluation. Desired return: 12.724, Desired horizon: 362.47. Total reward achieved: -193.698\n",
      "Iter: 440, Loss: 1.386\n",
      "Evaluation. Desired return: 18.371, Desired horizon: 371.78. Total reward achieved: -156.213\n",
      "Iter: 450, Loss: 1.386\n",
      "Evaluation. Desired return: 23.414, Desired horizon: 371.78. Total reward achieved: -527.650\n",
      "Iter: 460, Loss: 1.386\n",
      "Evaluation. Desired return: 14.453, Desired horizon: 390.57. Total reward achieved: -204.479\n",
      "Iter: 470, Loss: 1.387\n",
      "Evaluation. Desired return: 12.634, Desired horizon: 390.82. Total reward achieved: -451.758\n",
      "Iter: 480, Loss: 1.386\n",
      "Evaluation. Desired return: 24.197, Desired horizon: 399.53. Total reward achieved: -471.073\n",
      "Iter: 490, Loss: 1.386\n",
      "Evaluation. Desired return: 28.113, Desired horizon: 408.79. Total reward achieved: -197.313\n",
      "Iter: 500, Loss: 1.386\n",
      "Evaluation. Desired return: 19.638, Desired horizon: 408.78. Total reward achieved: -429.103\n",
      "Iter: 510, Loss: 1.386\n",
      "Evaluation. Desired return: 17.201, Desired horizon: 408.95. Total reward achieved: -527.181\n",
      "Iter: 520, Loss: 1.385\n",
      "Evaluation. Desired return: 22.536, Desired horizon: 408.95. Total reward achieved: -211.328\n",
      "Iter: 530, Loss: 1.386\n",
      "Evaluation. Desired return: 27.597, Desired horizon: 408.3. Total reward achieved: -73.535\n",
      "Iter: 540, Loss: 1.386\n",
      "Evaluation. Desired return: 16.981, Desired horizon: 408.0. Total reward achieved: -192.132\n",
      "Iter: 550, Loss: 1.386\n",
      "Evaluation. Desired return: 12.585, Desired horizon: 408.0. Total reward achieved: -138.798\n",
      "Iter: 560, Loss: 1.386\n",
      "Evaluation. Desired return: 13.090, Desired horizon: 408.0. Total reward achieved: -561.798\n",
      "Iter: 570, Loss: 1.386\n",
      "Evaluation. Desired return: 12.539, Desired horizon: 417.0. Total reward achieved: -202.853\n",
      "Iter: 580, Loss: 1.387\n",
      "Evaluation. Desired return: 21.457, Desired horizon: 435.31. Total reward achieved: -758.095\n",
      "Iter: 590, Loss: 1.386\n",
      "Evaluation. Desired return: 29.047, Desired horizon: 435.45. Total reward achieved: -211.489\n",
      "Iter: 600, Loss: 1.386\n",
      "Evaluation. Desired return: 30.537, Desired horizon: 445.12. Total reward achieved: -186.887\n",
      "Iter: 610, Loss: 1.386\n",
      "Evaluation. Desired return: 21.024, Desired horizon: 444.61. Total reward achieved: -173.665\n",
      "Iter: 620, Loss: 1.386\n",
      "Evaluation. Desired return: 32.399, Desired horizon: 453.64. Total reward achieved: -203.735\n",
      "Iter: 630, Loss: 1.386\n",
      "Evaluation. Desired return: 17.804, Desired horizon: 463.03. Total reward achieved: -205.638\n",
      "Iter: 640, Loss: 1.386\n",
      "Evaluation. Desired return: 16.101, Desired horizon: 472.32. Total reward achieved: -486.276\n",
      "Iter: 650, Loss: 1.386\n",
      "Evaluation. Desired return: 22.025, Desired horizon: 472.47. Total reward achieved: -339.273\n",
      "Iter: 660, Loss: 1.386\n",
      "Evaluation. Desired return: 16.144, Desired horizon: 472.47. Total reward achieved: -175.594\n",
      "Iter: 670, Loss: 1.386\n",
      "Evaluation. Desired return: 33.047, Desired horizon: 490.82. Total reward achieved: -481.139\n",
      "Iter: 680, Loss: 1.386\n",
      "Evaluation. Desired return: 35.145, Desired horizon: 490.82. Total reward achieved: -451.932\n",
      "Iter: 690, Loss: 1.387\n",
      "Evaluation. Desired return: 24.347, Desired horizon: 499.86. Total reward achieved: -157.930\n",
      "Iter: 700, Loss: 1.386\n",
      "Evaluation. Desired return: 19.419, Desired horizon: 499.7. Total reward achieved: -831.799\n",
      "Iter: 704, Loss: 1.386\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-47e0ca25709b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbehavior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstochastic_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUDRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3e924fa1ba33>\u001b[0m in \u001b[0;36mUDRL\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                           \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                           \u001b[0mn_episodes_per_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                           last_few)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ae56ab8c9480>\u001b[0m in \u001b[0;36mgenerate_episodes\u001b[0;34m(policy, buffer, n_episodes, last_few)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_few\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# See Algorithm 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7494851986ed>\u001b[0m in \u001b[0;36mgenerate_episode\u001b[0;34m(policy, state, command)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         action = policy(torch.FloatTensor(state), \n\u001b[0;32m---> 13\u001b[0;31m                         torch.FloatTensor(command))\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "behavior, buffer, stochastic_policy, greedy_policy = UDRL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent(greedy_policy, buffer, 1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_command(buffer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
