{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agents using Upside-Down Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... TODO: Intro ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rl_vs_udrl.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/toy_env.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from helpers import make_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import rocket_lander_gym\n",
    "\n",
    "env = gym.make('RocketLander-v0') # RocketLander-v0 | LunarLander-v2 | MountainCar-v0 | CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upside-Down RL Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of (input, target) pairs per batch used for training the behavior function\n",
    "batch_size = 128\n",
    "\n",
    "# Scaling factor for desired horizon input\n",
    "horizon_scale = 0.02\n",
    "\n",
    "# Number of episodes from the end of the replay buffer used for sampling exploratory\n",
    "# commands\n",
    "last_few = 100\n",
    "\n",
    "# Learning rate for the ADAM optimizer\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Number of exploratory episodes generated per step of UDRL training\n",
    "n_episodes_per_iter = 100\n",
    "\n",
    "# Number of gradient-based updates of the behavior function per step of UDRL training\n",
    "n_updates_per_iter = 15\n",
    "\n",
    "# Number of warm up episodes at the beginning of training\n",
    "n_warm_up_episodes = 50\n",
    "\n",
    "# Maximum size of the replay buffer (in episodes)\n",
    "replay_size = 700\n",
    "\n",
    "# Scaling factor for desired horizon input\n",
    "return_scale = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "RL does not explicitly maximize returns, but instead relies on exploration to continually discover higher return trajectories so that the behavior function can be trained on them. To drive learning progress, we found it helpful to use a replay buffer containing a fixed maximum number of trajectories with the highest returns seen so far, sorted in increasing order by return. The maximum buffer size is a hyperparameter. Since the agent starts learning with zero experience, an initial set of trajectories is generated by executing random actions in the environment. The trajectories are added to the replay buffer and used to start training the agent’s behavior function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: elaborate more\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, episode):\n",
    "        self.buffer.append(episode)\n",
    "    \n",
    "    def get_last(self, num):\n",
    "        return self.buffer[-num:]\n",
    "    \n",
    "    def sort_and_trim(self):\n",
    "        key_sort = lambda episode: episode.total_return\n",
    "        self.buffer = sorted(self.buffer, key=key_sort)[-self.size:]\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Function and Setup\n",
    "\n",
    "As described earlier, at any time t during an episode, the current behavior function B produces an action distribution in response to the current state st and command c<sub>t</sub> = (d<sup>r</sup><sub>t</sub>, d<sup>h</sup><sub>t</sub>), where d<sup>r</sup><sub>t</sub> ∈ R is the desired return and d<sup>h</sup><sub>t</sub> ∈ N is the desired time horizon at time t. The predicted action distribution _P_(a<sub>t</sub> | s<sub>t</sub>, c<sub>t</sub>) = _B_(s<sub>t</sub>, c<sub>t</sub>; θ), where θ denotes a vector of trainable parameters, is expected to lead to successful execution of the command ct interpreted as: “achieve a return d<sup>r</sup><sub>t</sub> during the next d<sup>h</sup><sub>t</sub> steps”. For a given initial command input c<sub>0</sub>, _B_ can be used to generate a trajectory using Algorithm 2 by sampling actions predicted for the current command and updating the command according to the obtained rewards and elapsed time.\n",
    "\n",
    "An important implementation detail is that d<sup>h</sup><sub>t</sub> is always set to max(d<sup>h</sup><sub>t</sub>, 1) such that it is a valid time horizon. Furthermore, d<sup>r</sup><sub>t</sub> is clipped such that it is upper-bounded by the maximum return achievable in the environment. This only affects agent evaluations (not training) and avoids situations where negative rewards (rt) can lead to desired returns that are not achievable from any state (see Algorithm 2; line 8).\n",
    "\n",
    "All agents were implemented using articial neural networks. The behavior function for UDRL agents was implemented using fully-connected feed-forward networks for LunarLander-v2, and convolutional neural networks (CNNs; 16) for TakeCover-v0. The command inputs were scaled by a fixed scaling factor, transformed by a fully-connected sigmoidal layer, and then multiplied element-wise with an embedding of the observed inputs (after the first layer for fully-connected networks; after all convolutional layers for CNNs). Apart from this small modification regarding UDRL command inputs, the network architectures were identical for all algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 hidden_size, \n",
    "                 command_scale = [1, 1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.command_scale = torch.FloatTensor(command_scale)\n",
    "        \n",
    "        self.state_fc = torch.nn.Sequential(torch.nn.Linear(state_size, \n",
    "                                                            hidden_size), \n",
    "                                            torch.nn.Sigmoid())\n",
    "        \n",
    "        self.command_fc = torch.nn.Sequential(torch.nn.Linear(2, hidden_size), \n",
    "                                              torch.nn.Sigmoid())\n",
    "        \n",
    "        self.output_fc = torch.nn.Sequential(torch.nn.Linear(hidden_size, \n",
    "                                                             hidden_size), \n",
    "                                             torch.nn.ReLU(), \n",
    "                                             torch.nn.Linear(hidden_size, \n",
    "                                                             action_size))\n",
    "        \n",
    "    \n",
    "    def forward(self, state, command):\n",
    "        state_output = self.state_fc(state)\n",
    "        command_output = self.command_fc(command * self.command_scale)\n",
    "        embedding = torch.mul(state_output, command_output)\n",
    "        return self.output_fc(embedding)\n",
    "    \n",
    "    def action(self, state, command):\n",
    "        logits = self.forward(state, command)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist.sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Behavior Function\n",
    "\n",
    "As discussed in Section 2.2, B admits supervised training on a large amount of input-target examples from any past episode. The goal of training is to make the behavior function produce outputs consistent with all previously recorded trajectories in the replay buffer according to Equation 1. \n",
    "\n",
    "To draw a training example from a random episode in the replay buffer, time step indices t1 and t2 are selected\n",
    "randomly such that 0 ≤ t1 < t2 ≤ T, where T is the length of the selected episode. Then the input for training _B_ is\n",
    "(s<sub>t1</sub>, (d<sup>r</sup>, d<sup>h</sup>)), where d<sup>r</sup> = ∑ r<sub>t</sub> and d<sup>h</sup> = t2 − t1, and the target is a<sub>t1</sub>, the action taken at t1. To summarize, the training examples are generated by selecting the time horizons, actions, observations and rewards in the past, and generating input-target pairs consistent with them.\n",
    "\n",
    "Several heuristics may be used to select and combine training examples into mini-batches for gradient-based SL. For\n",
    "all experiments in this paper, only \"trailing segments\" were sampled from each episode, i.e., we set t2 = T − 1 where\n",
    "T is the length of any episode. This discards a large amount of potential training examples but is a good fit for episodic tasks where the goal is to optimize the total reward until the end of each episode. It also makes training easier, since the behavior function only needs to learn to execute a subset of possible commands. To keep the setup simple, a fixed number of training iterations using Adam were performed in each training step for all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_behavior():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Exploratory Commands\n",
    "\n",
    "After each training phase, the agent can attempt to generate new, previously infeasible behavior, potentially achieving higher returns. To profit from such exploration through generalization, one must first create a set of new initial commands c0 to be used in Algorithm 2. We use the following procedure to sample commands:\n",
    "\n",
    "1. A number of episodes from the end of the replay buffer (i.e., with the highest returns) are selected. This number is a hyperparameter and remains fixed during training.\n",
    "\n",
    "2. The exploratory desired horizon d<sup>h</sup><sub>0</sub> is set to the mean of the lengths of the selected episodes.\n",
    "\n",
    "3. The exploratory desired returns d<sup>r</sup><sub>0</sub> are sampled from the uniform distribution U\\[M, M + S\\] where M is the mean and S is the standard deviation of the selected episodic returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_command(buffer, last_few):\n",
    "    if len(buffer) == 0: return [1, 1]\n",
    "    \n",
    "    # 1.\n",
    "    commands = buffer.get_last(last_few)\n",
    "    \n",
    "    # 2.\n",
    "    lengths = [command.length for command in commands]\n",
    "    desired_horizon = np.mean(lengths)\n",
    "    \n",
    "    # 3.\n",
    "    returns = [command.total_return for command in commands]\n",
    "    mean_return, std_return = np.mean(returns), np.std(returns)\n",
    "    desired_returns = np.random.uniform(mean_return, mean_return+std_return)\n",
    "    \n",
    "    return [desired_returns, desired_horizon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: Upside-Down Reinforcement Learning: High-level Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state = None, command = None):\n",
    "    return np.random.randint(env.action_space.n)\n",
    "\n",
    "def initialize_replay_buffer(replay_size, n_warm_up_episodes, last_few):\n",
    "    replay_buffer = ReplayBuffer(replay_size)\n",
    "    for i in range(n_warm_up_episodes):\n",
    "        state = env.reset().tolist()\n",
    "        command = sample_command(buffer, last_few)\n",
    "        episode = generate_episode(random_policy, state, command) # See Algorithm 2\n",
    "        replay_buffer.add(episode)\n",
    "    \n",
    "    return replay_buffer\n",
    "\n",
    "def initialize_behavior_function(state_size, action_size, hidden_size, command_scale):\n",
    "    return Behavior(state_size, \n",
    "                    action_size, \n",
    "                    hidden_size, \n",
    "                    command_scale)\n",
    "\n",
    "def stopping_criteria():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Generates an Episode using the Behavior Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavior_policy(state, command):\n",
    "    return behavior.action(state, command)\n",
    "\n",
    "def generate_episode(policy, state, command = [1, 1]):\n",
    "    desired_return = command[0]\n",
    "    desired_horizon = command[1]\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    time_steps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(state, command)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        state = next_state.tolist()\n",
    "        desired_return -= reward\n",
    "        desired_horizon -= 1\n",
    "        command = [desired_return, desired_horizon]\n",
    "        time_steps += 1\n",
    "        \n",
    "    return make_episode(states, actions, rewards, sum(rewards), time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
