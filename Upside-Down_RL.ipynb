{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agents using Upside-Down Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... TODO: Intro ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rl_vs_udrl.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/toy_env.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from helpers import make_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import rocket_lander_gym\n",
    "\n",
    "env = gym.make('RocketLander-v0') # RocketLander-v0 | LunarLander-v2 | MountainCar-v0 | CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upside-Down RL Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of (input, target) pairs per batch used for training the behavior function\n",
    "batch_size = 128\n",
    "\n",
    "# Scaling factor for desired horizon input\n",
    "horizon_scale = 0.02\n",
    "\n",
    "# Number of episodes from the end of the replay buffer used for sampling exploratory\n",
    "# commands\n",
    "last_few = 100\n",
    "\n",
    "# Learning rate for the ADAM optimizer\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Number of exploratory episodes generated per step of UDRL training\n",
    "n_episodes_per_iter = 100\n",
    "\n",
    "# Number of gradient-based updates of the behavior function per step of UDRL training\n",
    "n_updates_per_iter = 15\n",
    "\n",
    "# Number of warm up episodes at the beginning of training\n",
    "n_warm_up_episodes = 50\n",
    "\n",
    "# Maximum size of the replay buffer (in episodes)\n",
    "replay_size = 700\n",
    "\n",
    "# Scaling factor for desired horizon input\n",
    "return_scale = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Replay Buffer\n",
    "\n",
    "RL does not explicitly maximize returns, but instead relies on exploration to continually discover higher return trajectories so that the behavior function can be trained on them. To drive learning progress, we found it helpful to use a replay buffer containing a fixed maximum number of trajectories with the highest returns seen so far, sorted in increasing order by return. The maximum buffer size is a hyperparameter. Since the agent starts learning with zero experience, an initial set of trajectories is generated by executing random actions in the environment. The trajectories are added to the replay buffer and used to start training the agentâ€™s behavior function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I guess we need to get samples\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, episode):\n",
    "        self.buffer.append(episode)\n",
    "    \n",
    "    def get_last(self, num):\n",
    "        return self.buffer[-num:]\n",
    "    \n",
    "    def sort_and_trim(self):\n",
    "        key_sort = lambda episode: episode.total_return\n",
    "        self.buffer = sorted(self.buffer, key=key_sort)[-self.size:]\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4 Sampling Exploratory Commands\n",
    "\n",
    "After each training phase, the agent can attempt to generate new, previously infeasible behavior, potentially achieving higher returns. To profit from such exploration through generalization, one must first create a set of new initial commands c0 to be used in Algorithm 2. We use the following procedure to sample commands:\n",
    "\n",
    "1. A number of episodes from the end of the replay buffer (i.e., with the highest returns) are selected. This number is a hyperparameter and remains fixed during training.\n",
    "\n",
    "2. The exploratory desired horizon d<sup>h</sup><sub>0</sub> is set to the mean of the lengths of the selected episodes.\n",
    "\n",
    "3. The exploratory desired returns d<sup>r</sup><sub>0</sub> are sampled from the uniform distribution U\\[M, M + S\\] where M is the mean and S is the standard deviation of the selected episodic returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_command(buffer, last_few):\n",
    "    if len(buffer) == 0: return [1, 1]\n",
    "    \n",
    "    # 1.\n",
    "    commands = buffer.get_last(last_few)\n",
    "    \n",
    "    # 2.\n",
    "    lengths = [command.length for command in commands]\n",
    "    desired_horizon = np.mean(lengths)\n",
    "    \n",
    "    # 3.\n",
    "    returns = [command.total_return for command in commands]\n",
    "    mean_return, std_return = np.mean(returns), np.std(returns)\n",
    "    desired_returns = np.random.uniform(mean_return, mean_return+std_return)\n",
    "    \n",
    "    return [desired_returns, desired_horizon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Setup\n",
    "\n",
    "All agents were implemented using articial neural networks. The behavior function for UDRL agents was implemented using fully-connected feed-forward networks for LunarLander-v2, and convolutional neural networks (CNNs; 16) for TakeCover-v0. The command inputs were scaled by a fixed scaling factor, transformed by a fully-connected sigmoidal layer, and then multiplied element-wise with an embedding of the observed inputs (after the first layer for fully-connected networks; after all convolutional layers for CNNs). Apart from this small modification regarding UDRL command inputs, the network architectures were identical for all algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 hidden_size, \n",
    "                 command_scale = [1, 1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.command_scale = torch.FloatTensor(command_scale)\n",
    "        \n",
    "        self.state_fc = torch.nn.Sequential(torch.nn.Linear(state_size, \n",
    "                                                            hidden_size), \n",
    "                                            torch.nn.Sigmoid())\n",
    "        \n",
    "        self.command_fc = torch.nn.Sequential(torch.nn.Linear(2, hidden_size), \n",
    "                                              torch.nn.Sigmoid())\n",
    "        \n",
    "        self.output_fc = torch.nn.Sequential(torch.nn.Linear(hidden_size, \n",
    "                                                             hidden_size), \n",
    "                                             torch.nn.ReLU(), \n",
    "                                             torch.nn.Linear(hidden_size, \n",
    "                                                             action_size))\n",
    "        \n",
    "    \n",
    "    def forward(self, state, command):\n",
    "        state_output = self.state_fc(state)\n",
    "        command_output = self.command_fc(command * self.command_scale)\n",
    "        embedding = torch.mul(state_output, command_output)\n",
    "        return self.output_fc(embedding)\n",
    "    \n",
    "    def action(self, state, command):\n",
    "        logits = self.forward(state, command)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist.sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: Upside-Down Reinforcement Learning: High-level Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state = None, command = None):\n",
    "    return np.random.randint(env.action_space.n)\n",
    "\n",
    "def initialize_replay_buffer(replay_size, n_warm_up_episodes, last_few):\n",
    "    replay_buffer = ReplayBuffer(replay_size)\n",
    "    for i in range(n_warm_up_episodes):\n",
    "        state = env.reset().tolist()\n",
    "        command = sample_command(buffer, last_few)\n",
    "        episode = generate_episode(random_policy, state, command)\n",
    "        replay_buffer.add(episode)\n",
    "    \n",
    "    return replay_buffer\n",
    "\n",
    "def initialize_behavior_function(state_size, action_size, hidden_size, command_scale):\n",
    "    return Behavior(state_size, \n",
    "                    action_size, \n",
    "                    hidden_size, \n",
    "                        command_scale)\n",
    "\n",
    "def stopping_criteria():\n",
    "    pass\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Generates an Episode using the Behavior Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavior_policy(state, command):\n",
    "    return behavior.action(state, command)\n",
    "\n",
    "def generate_episode(policy, state, command = [1, 1]):\n",
    "    desired_return = command[0]\n",
    "    desired_horizon = command[1]\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    time_steps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(state, command)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        state = next_state.tolist()\n",
    "        desired_return -= reward\n",
    "        desired_horizon -= 1\n",
    "        command = [desired_return, desired_horizon]\n",
    "        time_steps += 1\n",
    "        \n",
    "    return make_episode(states, actions, rewards, sum(rewards), time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
