{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agents using Upside-Down Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... TODO: Intro ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_return_achievable = 1\n",
    "num_warm_up_episodes = 50\n",
    "horizon_scale = 0.02\n",
    "return_scale = 0.02\n",
    "buffer_size = 1000\n",
    "learning_rate = 0.002\n",
    "# TODO: find out more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_episode = namedtuple('Episode', \n",
    "                          field_names=['states', \n",
    "                                       'actions', \n",
    "                                       'rewards', \n",
    "                                       'total_return', \n",
    "                                       'length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Replay Buffer\n",
    "\n",
    "RL does not explicitly maximize returns, but instead relies on exploration to continually discover higher return trajectories so that the behavior function can be trained on them. To drive learning progress, we found it helpful to use a replay buffer containing a fixed maximum number of trajectories with the highest returns seen so far, sorted in increasing order by return. The maximum buffer size is a hyperparameter. Since the agent starts learning with zero experience, an initial set of trajectories is generated by executing random actions in the environment. The trajectories are added to the replay buffer and used to start training the agentâ€™s behavior function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I guess we need to get samples\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, episode):\n",
    "        self.buffer.append(episode)\n",
    "    \n",
    "    def sort_and_trim(self):\n",
    "        key_sort = lambda episode: episode.total_return\n",
    "        self.buffer = sorted(self.buffer, key=key_sort)[-self.max_size:]\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Setup\n",
    "\n",
    "All agents were implemented using articial neural networks. The behavior function for UDRL agents was implemented using fully-connected feed-forward networks for LunarLander-v2, and convolutional neural networks (CNNs; 16) for TakeCover-v0. The command inputs were scaled by a fixed scaling factor, transformed by a fully-connected sigmoidal layer, and then multiplied element-wise with an embedding of the observed inputs (after the first layer for fully-connected networks; after all convolutional layers for CNNs). Apart from this small modification regarding UDRL command inputs, the network architectures were identical for all algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 hidden_size, \n",
    "                 command_scale = [1, 1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.command_scale = torch.FloatTensor(command_scale)\n",
    "        \n",
    "        self.state_fc = torch.nn.Sequential(torch.nn.Linear(state_size, \n",
    "                                                            hidden_size), \n",
    "                                            torch.nn.Sigmoid())\n",
    "        \n",
    "        self.command_fc = torch.nn.Sequential(torch.nn.Linear(2, hidden_size), \n",
    "                                              torch.nn.Sigmoid())\n",
    "        \n",
    "        self.output_fc = torch.nn.Sequential(torch.nn.Linear(hidden_size, \n",
    "                                                             hidden_size), \n",
    "                                             torch.nn.ReLU(), \n",
    "                                             torch.nn.Linear(hidden_size, \n",
    "                                                             action_size))\n",
    "        \n",
    "    \n",
    "    def forward(self, state, command):\n",
    "        state_output = self.state_fc(state)\n",
    "        command_output = self.command_fc(command * self.command_scale)\n",
    "        embedding = torch.mul(state_output, command_output)\n",
    "        return self.output_fc(embedding)\n",
    "    \n",
    "    def action(self, state, command):\n",
    "        logits = self.forward(state, command)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: Upside-Down Reinforcement Learning: High-level Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_replay_buffer(behavior, buffer, num_episodes):\n",
    "    pass\n",
    "\n",
    "def initialize_behavior_function():\n",
    "    pass\n",
    "\n",
    "def stopping_criteria():\n",
    "    pass\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Generates an Episode using the Behavior Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(behavior, state, command = [1, 1]):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
