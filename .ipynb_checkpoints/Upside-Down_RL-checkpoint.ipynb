{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agents using Upside-Down Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... TODO: Intro ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from helpers import make_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upside-Down RL Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hyperparams.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "horizon_scale = 0.02\n",
    "last_few = 100\n",
    "learning_rate = 0.002\n",
    "n_episodes_per_iter = 100\n",
    "n_updates_per_iter = 15\n",
    "n_warm_up_episodes = 50\n",
    "replay_size = 700\n",
    "return_scale = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Replay Buffer\n",
    "\n",
    "RL does not explicitly maximize returns, but instead relies on exploration to continually discover higher return trajectories so that the behavior function can be trained on them. To drive learning progress, we found it helpful to use a replay buffer containing a fixed maximum number of trajectories with the highest returns seen so far, sorted in increasing order by return. The maximum buffer size is a hyperparameter. Since the agent starts learning with zero experience, an initial set of trajectories is generated by executing random actions in the environment. The trajectories are added to the replay buffer and used to start training the agentâ€™s behavior function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I guess we need to get samples\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, episode):\n",
    "        self.buffer.append(episode)\n",
    "    \n",
    "    def sort_and_trim(self):\n",
    "        key_sort = lambda episode: episode.total_return\n",
    "        self.buffer = sorted(self.buffer, key=key_sort)[-self.max_size:]\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4 Sampling Exploratory Commands\n",
    "\n",
    "After each training phase, the agent can attempt to generate new, previously infeasible behavior, potentially achieving higher returns. To profit from such exploration through generalization, one must first create a set of new initial commands c0 to be used in Algorithm 2. We use the following procedure to sample commands:\n",
    "\n",
    "1. A number of episodes from the end of the replay buffer (i.e., with the highest returns) are selected. This number is a hyperparameter and remains fixed during training.\n",
    "\n",
    "2. The exploratory desired horizon d<sup>h</sup><sub>0</sub> is set to the mean of the lengths of the selected episodes.\n",
    "\n",
    "3. The exploratory desired returns d<sup>r</sup><sub>0</sub> are sampled from the uniform distribution U\\[M, M + S\\] where M is the mean and S is the standard deviation of the selected episodic returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_command(buffer, num):\n",
    "    if len(buffer) == 0: return [1, 1]\n",
    "    \n",
    "    # 1.\n",
    "    commands = buffer[-num:]\n",
    "    \n",
    "    # 2.\n",
    "    lengths = [command.length for command in commands]\n",
    "    desired_horizon = np.mean(lengths)\n",
    "    \n",
    "    # 3.\n",
    "    returns = [command.total_return for command in commands]\n",
    "    mean_return, std_return = np.mean(returns), np.std(returns)\n",
    "    desired_returns = np.random.uniform(mean_return, mean_return+std_return)\n",
    "    \n",
    "    return [desired_returns, desired_horizon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Setup\n",
    "\n",
    "All agents were implemented using articial neural networks. The behavior function for UDRL agents was implemented using fully-connected feed-forward networks for LunarLander-v2, and convolutional neural networks (CNNs; 16) for TakeCover-v0. The command inputs were scaled by a fixed scaling factor, transformed by a fully-connected sigmoidal layer, and then multiplied element-wise with an embedding of the observed inputs (after the first layer for fully-connected networks; after all convolutional layers for CNNs). Apart from this small modification regarding UDRL command inputs, the network architectures were identical for all algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Behavior(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 hidden_size, \n",
    "                 command_scale = [1, 1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.command_scale = torch.FloatTensor(command_scale)\n",
    "        \n",
    "        self.state_fc = torch.nn.Sequential(torch.nn.Linear(state_size, \n",
    "                                                            hidden_size), \n",
    "                                            torch.nn.Sigmoid())\n",
    "        \n",
    "        self.command_fc = torch.nn.Sequential(torch.nn.Linear(2, hidden_size), \n",
    "                                              torch.nn.Sigmoid())\n",
    "        \n",
    "        self.output_fc = torch.nn.Sequential(torch.nn.Linear(hidden_size, \n",
    "                                                             hidden_size), \n",
    "                                             torch.nn.ReLU(), \n",
    "                                             torch.nn.Linear(hidden_size, \n",
    "                                                             action_size))\n",
    "        \n",
    "    \n",
    "    def forward(self, state, command):\n",
    "        state_output = self.state_fc(state)\n",
    "        command_output = self.command_fc(command * self.command_scale)\n",
    "        embedding = torch.mul(state_output, command_output)\n",
    "        return self.output_fc(embedding)\n",
    "    \n",
    "    def action(self, state, command):\n",
    "        logits = self.forward(state, command)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: Upside-Down Reinforcement Learning: High-level Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_replay_buffer(behavior, buffer, num_episodes):\n",
    "    pass\n",
    "\n",
    "def initialize_behavior_function():\n",
    "    pass\n",
    "\n",
    "def stopping_criteria():\n",
    "    pass\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Generates an Episode using the Behavior Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/udrl_algo2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(behavior, state, command = [1, 1]):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    steps = 0\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return make_episode(states, actions, rewards, sum(rewards), steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
